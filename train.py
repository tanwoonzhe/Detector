"""
æ¨¡å‹è®­ç»ƒè„šæœ¬
================================
è®­ç»ƒBTCè¶‹åŠ¿é¢„æµ‹æ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
    python train.py --model gru --epochs 100
    python train.py --model all --epochs 50
    python train.py --model cnn_lstm --use-hf-multi --interval 15min --epochs 100
    python train.py --use-binance-hist --interval 5min --days 365
"""

import argparse
import asyncio
import logging
import sys
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from config import ModelConfig, TradingConfig, FeatureConfig, APIConfig
from src.data_collection import CacheManager
from src.data_collection.coingecko_fetcher import CoinGeckoFetcher
from src.data_collection.fmp_fetcher import FMPFetcher
from src.data_collection.data_pipeline import DataPipeline
from src.data_collection.binance_historical import BinanceHistoricalFetcher, download_btc_historical, load_btc_historical
from src.data_collection.fred_fetcher import FREDFetcher
from src.data_collection.kaggle_fetcher import KaggleFetcher, load_kaggle_btc
from src.data_collection.hf_loader_multi import load_hf_btc_multi_granularity
from src.sentiment import SentimentAggregator
from src.features import FeatureEngineer
from src.validation import WalkForwardValidator, TimeSeriesMetrics
from src.models import (
    GRUPredictor, 
    BiLSTMPredictor, 
    CNNLSTMPredictor,
    LightGBMPredictor,
    ModelEnsemble
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def fetch_data(
    use_hf: bool = False, 
    merge_recent: bool = False, 
    use_fmp: bool = False, 
    fmp_days: int = 90,
    use_pipeline: bool = False,
    include_macro: bool = True,
    include_onchain: bool = True,
    use_hf_multi: bool = False,
    interval: str = "1h",
    use_binance_hist: bool = False,
    use_kaggle: bool = False,
    days: int = 90
):
    """
    è·å–è®­ç»ƒæ•°æ®
    
    Args:
        use_hf: ä½¿ç”¨HuggingFaceå†å²æ•°æ®é›†ï¼ˆå°æ—¶çº§ï¼‰
        merge_recent: åˆå¹¶æœ€è¿‘çš„CoinGeckoæ•°æ®ï¼ˆä¸use_hfä¸€èµ·ä½¿ç”¨ï¼‰
        use_fmp: ä½¿ç”¨Financial Modeling Prep (FMP) API
        fmp_days: FMPæ•°æ®å¤©æ•°
        use_pipeline: ä½¿ç”¨å¤šæ•°æ®æºç®¡é“ï¼ˆå®è§‚+é“¾ä¸Š+è·¨å¸‚åœºï¼‰
        include_macro: åŒ…å«å®è§‚æ•°æ®ï¼ˆéœ€è¦FMP APIï¼‰
        include_onchain: åŒ…å«é“¾ä¸Šæ•°æ®ï¼ˆCoinMetricsï¼‰
        use_hf_multi: ä½¿ç”¨å¤šç²’åº¦HuggingFaceæ•°æ®
        interval: æ•°æ®é—´éš” (1min, 5min, 15min, 30min, 1h, 4h, 1d)
        use_binance_hist: ä½¿ç”¨Binanceå†å²å½’æ¡£æ•°æ®
        use_kaggle: ä½¿ç”¨Kaggleå†å²æ•°æ®
        days: è·å–æ•°æ®çš„å¤©æ•°
    """
    logger.info("è·å–å†å²æ•°æ®...")
    
    df = None
    
    # é€‰é¡¹A: ä½¿ç”¨Binanceå†å²å½’æ¡£æ•°æ®ï¼ˆé«˜ä¼˜å…ˆçº§ï¼Œæœ€å®Œæ•´ï¼‰
    if use_binance_hist:
        logger.info(f"ğŸ“¥ ä½¿ç”¨ Binance å†å²å½’æ¡£æ•°æ® ({interval})...")
        try:
            # å…ˆå°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜
            df = load_btc_historical(interval=interval)
            
            if df is None or df.empty:
                # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œä¸‹è½½æ•°æ®
                logger.info("æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹ä¸‹è½½Binanceå†å²æ•°æ®...")
                df = asyncio.get_event_loop().run_until_complete(
                    download_btc_historical(interval=interval)
                )
            
            if df is not None and not df.empty:
                logger.info(f"âœ… Binanceå†å²æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} æ¡è®°å½•")
                logger.info(f"   æ—¶é—´èŒƒå›´: {df.index.min()} ~ {df.index.max()}")
                logger.info(f"   æ•°æ®é—´éš”: {interval}")
            else:
                logger.warning("âš ï¸ Binanceå†å²æ•°æ®ä¸ºç©ºï¼Œå›é€€åˆ°å…¶ä»–æ•°æ®æº")
                df = None
                
        except Exception as e:
            logger.error(f"âŒ Binanceå†å²æ•°æ®åŠ è½½å¼‚å¸¸: {e}")
            logger.info("å›é€€åˆ°å…¶ä»–æ•°æ®æº...")
            df = None
    
    # é€‰é¡¹B: ä½¿ç”¨å¤šç²’åº¦HuggingFaceæ•°æ®
    if df is None and use_hf_multi:
        logger.info(f"ğŸ“¥ åŠ è½½å¤šç²’åº¦ HuggingFace æ•°æ®é›† ({interval})...")
        try:
            df = load_hf_btc_multi_granularity(granularity=interval)
            
            if df is not None and not df.empty:
                logger.info(f"âœ… HFå¤šç²’åº¦æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} æ¡è®°å½•")
                logger.info(f"   æ—¶é—´èŒƒå›´: {df.index.min()} ~ {df.index.max()}")
                logger.info(f"   æ•°æ®é—´éš”: {interval}")
            else:
                logger.warning("âš ï¸ HFå¤šç²’åº¦æ•°æ®ä¸ºç©ºï¼Œå›é€€åˆ°å…¶ä»–æ•°æ®æº")
                df = None
                
        except Exception as e:
            logger.error(f"âŒ HFå¤šç²’åº¦æ•°æ®åŠ è½½å¼‚å¸¸: {e}")
            logger.info("å›é€€åˆ°å…¶ä»–æ•°æ®æº...")
            df = None
    
    # é€‰é¡¹C: ä½¿ç”¨Kaggleæ•°æ®
    if df is None and use_kaggle:
        logger.info("ğŸ“¥ åŠ è½½ Kaggle å†å²æ•°æ®...")
        try:
            resample_to = interval if interval in ["1min", "1h", "1d"] else "1h"
            df = load_kaggle_btc(resample_to=resample_to)
            
            if df is not None and not df.empty:
                logger.info(f"âœ… Kaggleæ•°æ®åŠ è½½æˆåŠŸ: {len(df)} æ¡è®°å½•")
                logger.info(f"   æ—¶é—´èŒƒå›´: {df.index.min()} ~ {df.index.max()}")
            else:
                logger.warning("âš ï¸ Kaggleæ•°æ®ä¸ºç©ºï¼Œå›é€€åˆ°å…¶ä»–æ•°æ®æº")
                df = None
                
        except Exception as e:
            logger.error(f"âŒ Kaggleæ•°æ®åŠ è½½å¼‚å¸¸: {e}")
            logger.info("å›é€€åˆ°å…¶ä»–æ•°æ®æº...")
            df = None
    
    # é€‰é¡¹0: ä½¿ç”¨å¤šæ•°æ®æºç®¡é“
    if use_pipeline:
        logger.info("ğŸ“¥ ä½¿ç”¨å¤šæ•°æ®æºç®¡é“è·å–æ•°æ®...")
        try:
            pipeline = DataPipeline(
                fmp_api_key=APIConfig.FMP_API_KEY,
                coinmetrics_api_key=getattr(APIConfig, 'COINMETRICS_API_KEY', '')
            )
            
            df = await pipeline.fetch_all(
                days=fmp_days,
                include_macro=include_macro and bool(APIConfig.FMP_API_KEY),
                include_onchain=include_onchain,
                include_cross_asset=True,
                resample_to_hourly=True
            )
            
            await pipeline.close()
            
            if not df.empty:
                logger.info(f"âœ… å¤šæºæ•°æ®åŠ è½½æˆåŠŸ: {len(df)} æ¡è®°å½•, {len(df.columns)} åˆ—")
                logger.info(f"   æ—¶é—´èŒƒå›´: {df.index.min()} ~ {df.index.max()}")
                return df
            else:
                logger.warning("âš ï¸ å¤šæºæ•°æ®ä¸ºç©ºï¼Œå›é€€åˆ°å…¶ä»–æ•°æ®æº")
                df = None
                
        except Exception as e:
            logger.error(f"âŒ å¤šæºæ•°æ®åŠ è½½å¼‚å¸¸: {e}")
            logger.info("å›é€€åˆ°å…¶ä»–æ•°æ®æº...")
            df = None
    
    # é€‰é¡¹1: ä½¿ç”¨ FMP æ•°æ®
    if df is None and use_fmp:
        logger.info("ğŸ“¥ ä½¿ç”¨ FMP API è·å–æ•°æ®...")
        try:
            api_key = APIConfig.FMP_API_KEY
            if not api_key:
                logger.warning("âš ï¸ FMP_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")
                raise ValueError("FMP APIå¯†é’¥æœªè®¾ç½®")
            
            fetcher = FMPFetcher(api_key=api_key)
            market_data = await fetcher.get_hourly_ohlcv(
                symbol="BTCUSD",
                days=fmp_days
            )
            await fetcher.close()
            
            df = market_data.to_dataframe()
            
            if not df.empty:
                logger.info(f"âœ… FMPæ•°æ®åŠ è½½æˆåŠŸ: {len(df)} æ¡è®°å½•")
                logger.info(f"   æ—¶é—´èŒƒå›´: {df.index.min()} ~ {df.index.max()}")
            else:
                logger.warning("âš ï¸ FMPæ•°æ®ä¸ºç©ºï¼Œå›é€€åˆ°å…¶ä»–æ•°æ®æº")
                df = None
                
        except Exception as e:
            logger.error(f"âŒ FMPæ•°æ®åŠ è½½å¼‚å¸¸: {e}")
            logger.info("å›é€€åˆ°å…¶ä»–æ•°æ®æº...")
            df = None
    
    # é€‰é¡¹2: ä½¿ç”¨ HuggingFace å†å²æ•°æ®
    if df is None and use_hf:
        logger.info("ğŸ“¥ åŠ è½½ HuggingFace å†å²æ•°æ®é›†...")
        try:
            from src.data_collection.hf_loader_fixed import load_hf_btc_data
            df = load_hf_btc_data()
            
            if not df.empty:
                logger.info(f"âœ… HFæ•°æ®åŠ è½½æˆåŠŸ: {len(df)} æ¡è®°å½•")
                logger.info(f"   æ—¶é—´èŒƒå›´: {df.index.min()} ~ {df.index.max()}")
                
                # å¦‚æœéœ€è¦åˆå¹¶æœ€æ–°æ•°æ®
                if merge_recent:
                    logger.info("ğŸ“Š åˆå¹¶æœ€æ–° CoinGecko æ•°æ®...")
                    fetcher = CoinGeckoFetcher()
                    recent_data = await fetcher.get_hourly_ohlcv(
                        symbol="bitcoin",
                        vs_currency="usd",
                        days=7  # è·å–æœ€è¿‘7å¤©æ•°æ®
                    )
                    await fetcher.close()
                    
                    df_recent = recent_data.to_dataframe()
                    
                    # ç»Ÿä¸€æ—¶åŒºå¤„ç†ï¼šç§»é™¤æ—¶åŒºä¿¡æ¯è¿›è¡Œæ¯”è¾ƒ
                    df_max_time = df.index.max()
                    if isinstance(df.index, pd.DatetimeIndex):
                        if df.index.tz is not None:
                            df.index = df.index.tz_localize(None)  # type: ignore
                    if isinstance(df_recent.index, pd.DatetimeIndex):
                        if df_recent.index.tz is not None:
                            df_recent.index = df_recent.index.tz_localize(None)  # type: ignore
                    if isinstance(df_max_time, pd.Timestamp) and df_max_time.tz is not None:
                        df_max_time = df_max_time.tz_localize(None)
                    
                    # åªä¿ç•™ HF æ•°æ®ä¹‹åçš„éƒ¨åˆ†
                    df_recent = df_recent[df_recent.index > df_max_time]
                    
                    if not df_recent.empty:
                        logger.info(f"   æ–°å¢ {len(df_recent)} æ¡æœ€æ–°æ•°æ®")
                        df = pd.concat([df, df_recent]).sort_index()
                    
            else:
                logger.warning("âš ï¸ HFæ•°æ®åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ° CoinGecko")
                df = None
                
        except Exception as e:
            logger.error(f"âŒ HFæ•°æ®åŠ è½½å¼‚å¸¸: {e}")
            df = None
    
    # å¦‚æœæ²¡æœ‰ä½¿ç”¨HFæˆ–HFåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ CoinGecko
    if df is None or df.empty:
        logger.info("ğŸ“Š ä½¿ç”¨ CoinGecko è·å–90å¤©å°æ—¶æ•°æ®...")
        fetcher = CoinGeckoFetcher()
        
        market_data = await fetcher.get_hourly_ohlcv(
            symbol="bitcoin",
            vs_currency="usd",
            days=90  # 90å¤©æ•°æ®ï¼Œçº¦2160æ¡
        )
        
        # è½¬æ¢ä¸ºDataFrame
        df = market_data.to_dataframe()
        logger.info(f"åŸå§‹æ•°æ®: {len(df)} æ¡ (èŒƒå›´: {df.index.min()} ~ {df.index.max()})")
        
        await fetcher.close()
    
    # ç¡®ä¿æ—¶åŒºä¸€è‡´
    if hasattr(df.index, 'tz'):
        if df.index.tz is None:  # type: ignore
            df.index = df.index.tz_localize('UTC')  # type: ignore
        else:
            df.index = df.index.tz_convert('UTC')  # type: ignore
    
    return df


def prepare_data(df: pd.DataFrame):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    logger.info(f"ç‰¹å¾å·¥ç¨‹å¼€å§‹... åˆå§‹æ•°æ®: {len(df)} è¡Œ")
    
    engineer = FeatureEngineer()
    
    # åˆ›å»ºç‰¹å¾
    df_features = engineer.create_features(df)
    logger.info(f"ç‰¹å¾åˆ›å»ºå: {len(df_features)} è¡Œ")
    
    if len(df_features) < 50:
        logger.error(f"ç‰¹å¾å·¥ç¨‹åæ•°æ®ä¸è¶³: {len(df_features)} è¡Œ < 50 è¡Œæœ€å°è¦æ±‚")
        raise ValueError(
            f"ç‰¹å¾å·¥ç¨‹åä»…å‰© {len(df_features)} è¡Œæ•°æ®ï¼Œä¸è¶³ä»¥è®­ç»ƒã€‚"
            f"å»ºè®®ï¼š1) ä½¿ç”¨æ›´å¤šå¤©æ•°çš„æ•°æ® 2) å‡å°ç‰¹å¾çª—å£å¤§å°ï¼ˆå½“å‰SEQUENCE_LENGTH={ModelConfig.SEQUENCE_LENGTH}ï¼‰"
        )
    
    # åˆ›å»ºæ ‡ç­¾
    df_features = engineer.create_labels(df_features)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X, y, feature_names = engineer.prepare_training_data(
        df_features, 
        target_window=1,  # 1å°æ—¶é¢„æµ‹
        for_classification=True
    )
    
    # åˆ›å»ºåºåˆ—
    X_seq, y_seq = engineer.create_sequences(X, y)
    
    logger.info(f"ç‰¹å¾ç»´åº¦: {X_seq.shape}")
    logger.info(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_seq.astype(int))}")
    
    return X_seq, y_seq, feature_names


def train_gru(X_train, y_train, X_val, y_val):
    """è®­ç»ƒGRUæ¨¡å‹"""
    logger.info("è®­ç»ƒGRUæ¨¡å‹...")
    
    model = GRUPredictor(
        hidden_size=ModelConfig.GRU_HIDDEN_SIZE,
        num_layers=ModelConfig.GRU_NUM_LAYERS,
        dropout=ModelConfig.DROPOUT,
        epochs=ModelConfig.EPOCHS,
        batch_size=ModelConfig.BATCH_SIZE,
        learning_rate=ModelConfig.LEARNING_RATE
    )
    
    model.build(input_shape=(X_train.shape[1], X_train.shape[2]), n_classes=3)
    history = model.train(X_train, y_train, X_val, y_val)
    
    return model, history


def train_bilstm(X_train, y_train, X_val, y_val):
    """è®­ç»ƒBiLSTMæ¨¡å‹"""
    logger.info("è®­ç»ƒBiLSTMæ¨¡å‹...")
    
    model = BiLSTMPredictor(
        hidden_size=ModelConfig.LSTM_HIDDEN_SIZE,
        num_layers=ModelConfig.LSTM_NUM_LAYERS,
        dropout=ModelConfig.DROPOUT,
        epochs=ModelConfig.EPOCHS,
        batch_size=ModelConfig.BATCH_SIZE
    )
    
    model.build(input_shape=(X_train.shape[1], X_train.shape[2]), n_classes=3)
    history = model.train(X_train, y_train, X_val, y_val)
    
    return model, history


def train_cnn_lstm(X_train, y_train, X_val, y_val):
    """è®­ç»ƒCNN-LSTMæ¨¡å‹"""
    logger.info("è®­ç»ƒCNN-LSTMæ¨¡å‹...")
    
    model = CNNLSTMPredictor(
        cnn_filters=64,
        kernel_sizes=[3, 5, 7],
        lstm_hidden=ModelConfig.LSTM_HIDDEN_SIZE,
        lstm_layers=2,
        dropout=ModelConfig.DROPOUT,
        epochs=ModelConfig.EPOCHS,
        batch_size=ModelConfig.BATCH_SIZE
    )
    
    model.build(input_shape=(X_train.shape[1], X_train.shape[2]), n_classes=3)
    history = model.train(X_train, y_train, X_val, y_val)
    
    return model, history


def train_lightgbm(X_train, y_train, X_val, y_val):
    """è®­ç»ƒLightGBMæ¨¡å‹"""
    logger.info("è®­ç»ƒLightGBMæ¨¡å‹...")
    
    model = LightGBMPredictor(
        n_estimators=500,
        max_depth=6,
        learning_rate=0.05
    )
    
    model.build()
    history = model.train(X_train, y_train, X_val, y_val)
    
    return model, history


def evaluate_model(model, X_test, y_test, name: str):
    """è¯„ä¼°æ¨¡å‹"""
    logger.info(f"è¯„ä¼° {name}...")
    
    y_pred = model.predict(X_test)
    metrics = TimeSeriesMetrics.calculate_metrics(y_test, y_pred)
    
    logger.info(f"  å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    logger.info(f"  F1åˆ†æ•°: {metrics['f1_macro']:.4f}")
    
    return metrics


def walk_forward_validation(X, y, model_class, model_kwargs):
    """Walk-ForwardéªŒè¯"""
    logger.info("æ‰§è¡ŒWalk-ForwardéªŒè¯...")
    
    validator = WalkForwardValidator(
        train_size=168 * 4,  # 4å‘¨è®­ç»ƒ
        test_size=168,       # 1å‘¨æµ‹è¯•
        step_size=24,        # æ¯å¤©æ»šåŠ¨
        expanding=True
    )
    
    all_metrics = []
    
    for fold, (train_idx, test_idx) in enumerate(validator.split(X)):
        logger.info(f"  Fold {fold + 1}...")
        
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # è®­ç»ƒæ¨¡å‹
        model = model_class(**model_kwargs)
        model.build(input_shape=(X_train.shape[1], X_train.shape[2]), n_classes=3)
        model.train(X_train, y_train)
        
        # è¯„ä¼°
        y_pred = model.predict(X_test)
        metrics = TimeSeriesMetrics.calculate_metrics(y_test, y_pred)
        all_metrics.append(metrics)
        
        if fold >= 4:  # é™åˆ¶foldæ•°é‡
            break
    
    # å¹³å‡æŒ‡æ ‡
    avg_metrics = {
        key: np.mean([m[key] for m in all_metrics])
        for key in all_metrics[0].keys()
    }
    
    logger.info(f"  å¹³å‡å‡†ç¡®ç‡: {avg_metrics['accuracy']:.4f}")
    logger.info(f"  å¹³å‡F1: {avg_metrics['f1_macro']:.4f}")
    
    return avg_metrics


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒBTCè¶‹åŠ¿é¢„æµ‹æ¨¡å‹')
    parser.add_argument('--model', type=str, default='gru',
                       choices=['gru', 'bilstm', 'cnn_lstm', 'lightgbm', 'all'],
                       help='è¦è®­ç»ƒçš„æ¨¡å‹')
    parser.add_argument('--epochs', type=int, default=ModelConfig.EPOCHS,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=ModelConfig.BATCH_SIZE,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--validate', action='store_true',
                       help='æ˜¯å¦æ‰§è¡ŒWalk-ForwardéªŒè¯')
    
    # === ä¼ ç»Ÿæ•°æ®æº ===
    parser.add_argument('--use-hf', action='store_true',
                       help='ä½¿ç”¨HuggingFaceå†å²æ•°æ®é›†ï¼ˆå°æ—¶çº§ï¼‰')
    parser.add_argument('--merge-recent', action='store_true',
                       help='åˆå¹¶æœ€è¿‘çš„CoinGeckoæ•°æ®ï¼ˆä¸--use-hfä¸€èµ·ä½¿ç”¨ï¼‰')
    parser.add_argument('--use-fmp', action='store_true',
                       help='ä½¿ç”¨Financial Modeling Prep (FMP) APIè·å–æ•°æ®')
    parser.add_argument('--fmp-days', type=int, default=90,
                       help='æ•°æ®å¤©æ•°ï¼ˆé»˜è®¤90å¤©ï¼‰')
    
    # === æ–°å¢é•¿å†å²æ•°æ®æº ===
    parser.add_argument('--use-hf-multi', action='store_true',
                       help='ä½¿ç”¨å¤šç²’åº¦HuggingFaceæ•°æ®ï¼ˆæ”¯æŒ1min/5min/15min/30min/1h/4h/1dï¼‰')
    parser.add_argument('--use-binance-hist', action='store_true',
                       help='ä½¿ç”¨Binanceå†å²å½’æ¡£æ•°æ®ï¼ˆ2017è‡³ä»Šï¼Œå®˜æ–¹æ•°æ®ï¼‰')
    parser.add_argument('--use-kaggle', action='store_true',
                       help='ä½¿ç”¨Kaggle BTCå†å²æ•°æ®ï¼ˆ2012è‡³ä»Šï¼‰')
    parser.add_argument('--interval', type=str, default='1h',
                       choices=['1min', '5min', '15min', '30min', '1h', '4h', '1d'],
                       help='æ•°æ®é—´éš”/ç²’åº¦ï¼ˆé»˜è®¤1hï¼‰')
    parser.add_argument('--days', type=int, default=365,
                       help='è·å–æ•°æ®å¤©æ•°ï¼ˆé»˜è®¤365å¤©ï¼‰')
    
    # === å¤šæ•°æ®æºç®¡é“å‚æ•° ===
    parser.add_argument('--use-pipeline', action='store_true',
                       help='ä½¿ç”¨å¤šæ•°æ®æºç®¡é“ï¼ˆåˆå¹¶å®è§‚+é“¾ä¸Š+è·¨å¸‚åœºæ•°æ®ï¼‰')
    parser.add_argument('--include-macro', action='store_true', default=True,
                       help='åŒ…å«å®è§‚ç»æµæ•°æ®ï¼ˆéœ€è¦FMP APIï¼‰')
    parser.add_argument('--include-onchain', action='store_true', default=True,
                       help='åŒ…å«é“¾ä¸Šæ•°æ®ï¼ˆCoinMetricsï¼‰')
    parser.add_argument('--no-macro', action='store_true',
                       help='ä¸åŒ…å«å®è§‚æ•°æ®')
    parser.add_argument('--no-onchain', action='store_true',
                       help='ä¸åŒ…å«é“¾ä¸Šæ•°æ®')
    
    args = parser.parse_args()
    
    # å¤„ç†å‚æ•°
    include_macro = not args.no_macro
    include_onchain = not args.no_onchain
    
    # æ›´æ–°é…ç½®
    ModelConfig.EPOCHS = args.epochs
    ModelConfig.BATCH_SIZE = args.batch_size
    
    logger.info("="*50)
    logger.info("BTCè¶‹åŠ¿é¢„æµ‹æ¨¡å‹è®­ç»ƒ")
    logger.info("="*50)
    
    # æ˜¾ç¤ºæ•°æ®æºé€‰æ‹©
    if args.use_binance_hist:
        logger.info(f"ğŸ“Š æ•°æ®æº: Binanceå†å²å½’æ¡£ (é—´éš”: {args.interval}, {args.days}å¤©)")
    elif args.use_hf_multi:
        logger.info(f"ğŸ“Š æ•°æ®æº: HuggingFaceå¤šç²’åº¦ (é—´éš”: {args.interval})")
    elif args.use_kaggle:
        logger.info(f"ğŸ“Š æ•°æ®æº: Kaggleå†å²æ•°æ®")
    elif args.use_pipeline:
        sources = ["BTCä»·æ ¼"]
        if include_macro and APIConfig.FMP_API_KEY:
            sources.append("å®è§‚ç»æµ")
        if include_onchain:
            sources.append("é“¾ä¸Šæ•°æ®")
        sources.append("è·¨å¸‚åœºèµ„äº§")
        logger.info(f"ğŸ“Š æ•°æ®æº: å¤šæºç®¡é“ ({', '.join(sources)})")
        logger.info(f"   æ•°æ®å¤©æ•°: {args.fmp_days}å¤©")
    elif args.use_fmp:
        logger.info(f"ğŸ“Š æ•°æ®æº: FMP ({args.fmp_days}å¤©)")
    elif args.use_hf:
        logger.info("ğŸ“Š æ•°æ®æº: HuggingFace" + (" + CoinGeckoæœ€æ–°æ•°æ®" if args.merge_recent else ""))
    else:
        logger.info("ğŸ“Š æ•°æ®æº: CoinGecko (90å¤©)")
    
    # è·å–æ•°æ®
    try:
        df = asyncio.run(fetch_data(
            use_hf=args.use_hf, 
            merge_recent=args.merge_recent,
            use_fmp=args.use_fmp,
            fmp_days=args.fmp_days,
            use_pipeline=args.use_pipeline,
            include_macro=include_macro,
            include_onchain=include_onchain,
            use_hf_multi=args.use_hf_multi,
            interval=args.interval,
            use_binance_hist=args.use_binance_hist,
            use_kaggle=args.use_kaggle,
            days=args.days
        ))
    except Exception as e:
        logger.error(f"è·å–æ•°æ®å¤±è´¥: {e}")
        logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        np.random.seed(42)
        dates = pd.date_range(end=datetime.now(), periods=2160, freq='H')
        returns = np.random.randn(2160) * 0.01
        prices = 65000 * np.cumprod(1 + returns)
        
        df = pd.DataFrame({
            'open': prices * (1 - np.random.rand(2160) * 0.005),
            'high': prices * (1 + np.random.rand(2160) * 0.01),
            'low': prices * (1 - np.random.rand(2160) * 0.01),
            'close': prices,
            'volume': np.random.rand(2160) * 1e9
        }, index=dates)
    
    # å‡†å¤‡æ•°æ®
    X_seq, y_seq, feature_names = prepare_data(df)
    
    # åˆ†å‰²è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    n = len(X_seq)
    train_end = int(n * 0.7)
    val_end = int(n * 0.85)
    
    X_train = X_seq[:train_end]
    y_train = y_seq[:train_end]
    X_val = X_seq[train_end:val_end]
    y_val = y_seq[train_end:val_end]
    X_test = X_seq[val_end:]
    y_test = y_seq[val_end:]
    
    logger.info(f"è®­ç»ƒé›†: {len(X_train)}, éªŒè¯é›†: {len(X_val)}, æµ‹è¯•é›†: {len(X_test)}")
    
    # ä¿å­˜æ¨¡å‹çš„ç›®å½•
    model_dir = Path(__file__).parent / 'models' / 'saved'
    model_dir.mkdir(parents=True, exist_ok=True)
    
    models = {}
    
    # è®­ç»ƒæ¨¡å‹
    if args.model in ['gru', 'all']:
        model, _ = train_gru(X_train, y_train, X_val, y_val)
        models['gru'] = model
        evaluate_model(model, X_test, y_test, 'GRU')
        model.save(model_dir / 'gru_best.pth')  # ä¿®æ­£ä¸º dashboard æœŸæœ›çš„åç§°
        logger.info(f"âœ… GRU æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir / 'gru_best.pth'}")
    
    if args.model in ['bilstm', 'all']:
        model, _ = train_bilstm(X_train, y_train, X_val, y_val)
        models['bilstm'] = model
        evaluate_model(model, X_test, y_test, 'BiLSTM')
        model.save(model_dir / 'bilstm_best.pth')
        logger.info(f"âœ… BiLSTM æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir / 'bilstm_best.pth'}")
    
    if args.model in ['cnn_lstm', 'all']:
        model, _ = train_cnn_lstm(X_train, y_train, X_val, y_val)
        models['cnn_lstm'] = model
        evaluate_model(model, X_test, y_test, 'CNN-LSTM')
        model.save(model_dir / 'cnn_lstm_best.pth')
        logger.info(f"âœ… CNN-LSTM æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir / 'cnn_lstm_best.pth'}")
    
    if args.model in ['lightgbm', 'all']:
        model, _ = train_lightgbm(X_train, y_train, X_val, y_val)
        models['lightgbm'] = model
        evaluate_model(model, X_test, y_test, 'LightGBM')
        model.save(model_dir / 'lightgbm_best.txt')  # ä¿®æ­£ä¸º dashboard æœŸæœ›çš„åç§°
        logger.info(f"âœ… LightGBM æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir / 'lightgbm_best.txt'}")
    
    # é›†æˆæ¨¡å‹
    if args.model == 'all' and len(models) > 1:
        logger.info("åˆ›å»ºé›†æˆæ¨¡å‹...")
        ensemble = ModelEnsemble(
            models=list(models.values()),
            strategy='soft_voting'
        )
        
        # è¯„ä¼°é›†æˆ
        y_pred = ensemble.predict(X_test)
        metrics = TimeSeriesMetrics.calculate_metrics(y_test, y_pred)
        logger.info(f"é›†æˆå‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
        logger.info(f"é›†æˆF1: {metrics['f1_macro']:.4f}")
    
    # Walk-ForwardéªŒè¯
    if args.validate:
        logger.info("\næ‰§è¡ŒWalk-ForwardéªŒè¯...")
        wf_metrics = walk_forward_validation(
            X_seq, y_seq,
            GRUPredictor,
            {'hidden_size': 128, 'num_layers': 2, 'epochs': 50}
        )
    
    logger.info("\n" + "="*50)
    logger.info("è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir}")
    logger.info("="*50)


if __name__ == "__main__":
    main()
