"""
HuggingFace å¤šç²’åº¦æ•°æ®åŠ è½½å™¨
================================
æ”¯æŒå°†HuggingFaceæ•°æ®é›†é‡é‡‡æ ·ä¸ºä¸åŒæ—¶é—´ç²’åº¦

æ”¯æŒçš„ç²’åº¦:
- 1min: åŸå§‹åˆ†é’Ÿçº§æ•°æ®
- 5min, 15min, 30min: çŸ­æœŸåˆ†æ
- 1h, 4h: ä¸­æœŸåˆ†æ  
- 1d: é•¿æœŸåˆ†æ

ä½¿ç”¨æ–¹æ³•:
    df_15min = load_hf_btc_multi_granularity(granularity="15min")
    df_1h = load_hf_btc_multi_granularity(granularity="1h")
"""

import pandas as pd
from pathlib import Path
from typing import Optional, Literal
import logging

try:
    from tqdm.auto import tqdm
except ImportError:
    tqdm = None

logger = logging.getLogger(__name__)

# æ”¯æŒçš„æ—¶é—´ç²’åº¦
VALID_GRANULARITIES = ["1min", "5min", "15min", "30min", "1h", "4h", "1d"]

# ç²’åº¦åˆ°pandasé¢‘ç‡çš„æ˜ å°„
GRANULARITY_TO_FREQ = {
    "1min": "min",
    "5min": "5min", 
    "15min": "15min",
    "30min": "30min",
    "1h": "h",
    "4h": "4h",
    "1d": "D"
}


def load_hf_btc_multi_granularity(
    granularity: str = "1h",
    cache_dir: Optional[Path] = None,
    force_reload: bool = False,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> pd.DataFrame:
    """
    ä»HuggingFaceæ•°æ®é›†åŠ è½½BTCå†å²æ•°æ®ï¼Œæ”¯æŒå¤šç§æ—¶é—´ç²’åº¦
    
    Args:
        granularity: æ—¶é—´ç²’åº¦ ("1min", "5min", "15min", "30min", "1h", "4h", "1d")
        cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º data/hf_cache/
        force_reload: å¼ºåˆ¶é‡æ–°åŠ è½½ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        
    Returns:
        DataFrame with columns: open, high, low, close, volume (index: timestamp)
    """
    if granularity not in VALID_GRANULARITIES:
        raise ValueError(f"æ— æ•ˆçš„æ—¶é—´ç²’åº¦: {granularity}ï¼Œæ”¯æŒ: {VALID_GRANULARITIES}")
    
    if cache_dir is None:
        cache_dir = Path(__file__).parent.parent.parent / "data" / "hf_cache"
    
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    cache_file = cache_dir / f"hf_btc_{granularity}.parquet"
    raw_cache = cache_dir / "hf_btc_raw.parquet"
    
    # å°è¯•ä»ç¼“å­˜åŠ è½½
    if not force_reload and cache_file.exists():
        logger.info(f"ğŸ“ ä»ç¼“å­˜åŠ è½½ {granularity} æ•°æ®: {cache_file}")
        df = pd.read_parquet(cache_file)
        
        # æ—¥æœŸè¿‡æ»¤
        if start_date:
            df = df[df.index >= start_date]
        if end_date:
            df = df[df.index <= end_date]
        
        logger.info(f"   {len(df)} æ¡è®°å½•, {df.index.min()} ~ {df.index.max()}")
        return df
    
    # åŠ è½½åŸå§‹æ•°æ®
    df_raw = _load_raw_hf_data(raw_cache, force_reload)
    
    if df_raw.empty:
        return pd.DataFrame()
    
    # é‡é‡‡æ ·åˆ°ç›®æ ‡ç²’åº¦
    df_resampled = _resample_data(df_raw, granularity)
    
    # ä¿å­˜ç¼“å­˜
    df_resampled.to_parquet(cache_file)
    logger.info(f"âœ… å·²ç¼“å­˜åˆ°: {cache_file}")
    
    # æ—¥æœŸè¿‡æ»¤
    if start_date:
        df_resampled = df_resampled[df_resampled.index >= start_date]
    if end_date:
        df_resampled = df_resampled[df_resampled.index <= end_date]
    
    return df_resampled


def _load_raw_hf_data(cache_path: Path, force_reload: bool = False) -> pd.DataFrame:
    """åŠ è½½åŸå§‹HFæ•°æ®ï¼ˆåˆ†é’Ÿçº§ï¼‰"""
    
    # æ£€æŸ¥ç¼“å­˜
    if not force_reload and cache_path.exists():
        logger.info(f"ğŸ“ ä»ç¼“å­˜åŠ è½½åŸå§‹æ•°æ®: {cache_path}")
        return pd.read_parquet(cache_path)
    
    logger.info("ğŸ“¥ é¦–æ¬¡åŠ è½½HFæ•°æ®é›†ï¼Œéœ€è¦ä¸‹è½½...")
    logger.info("âš ï¸ æ³¨æ„: æ•°æ®é›†è¾ƒå¤§(çº¦2.26Mè¡Œ)ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
    
    try:
        from datasets import load_dataset
    except ImportError:
        logger.error("è¯·å…ˆå®‰è£… datasets åº“: pip install datasets")
        return pd.DataFrame()
    
    try:
        logger.info("åŠ è½½ HuggingFace æ•°æ®é›†...")
        ds = load_dataset(
            "WinkingFace/CryptoLM-Bitcoin-BTC-USDT",
            split="train",
            streaming=False
        )
        # å¤„ç†ä¸åŒçš„è¿”å›ç±»å‹
        if hasattr(ds, 'to_pandas'):
            df: pd.DataFrame = ds.to_pandas()  # type: ignore
        else:
            # å¦‚æœæ˜¯DatasetDictï¼Œå–ç¬¬ä¸€ä¸ªsplit
            df = pd.DataFrame(ds)  # type: ignore
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(df)} è¡ŒåŸå§‹æ•°æ®")
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return pd.DataFrame()
    
    # åˆ—åæ˜ å°„
    rename_map = {}
    for col in df.columns:
        lc = col.lower()
        if lc in ["ts", "time", "timestamp", "date"]:
            rename_map[col] = "timestamp"
        elif lc == "open":
            rename_map[col] = "open"
        elif lc == "high":
            rename_map[col] = "high"
        elif lc == "low":
            rename_map[col] = "low"
        elif lc in ["close", "price"]:
            rename_map[col] = "close"
        elif lc in ["volume", "vol"]:
            rename_map[col] = "volume"
    
    df = df.rename(columns=rename_map)
    
    # åªä¿ç•™OHLCVåˆ—
    base_cols = [c for c in ["timestamp", "open", "high", "low", "close", "volume"] if c in df.columns]
    df = df[base_cols]
    
    # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
    required = {"timestamp", "open", "high", "low", "close"}
    if not required.issubset(df.columns):
        raise ValueError(f"æ•°æ®é›†ç¼ºå°‘å¿…è¦åˆ—ã€‚å½“å‰åˆ—: {df.columns.tolist()}")
    
    # æ•°æ®æ¸…æ´—
    for col in ["open", "high", "low", "close", "volume"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    
    # æ—¶é—´æˆ³å¤„ç†
    df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True, errors="coerce")
    df = df.dropna(subset=["timestamp", "open", "high", "low", "close"])
    df = df.sort_values("timestamp").set_index("timestamp")
    
    # ç§»é™¤æ—¶åŒº
    if isinstance(df.index, pd.DatetimeIndex) and df.index.tz is not None:
        df.index = df.index.tz_localize(None)  # type: ignore
    
    # ä¿å­˜åŸå§‹ç¼“å­˜
    cache_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(cache_path)
    logger.info(f"âœ… åŸå§‹æ•°æ®å·²ç¼“å­˜: {cache_path}")
    
    return df


def _resample_data(df: pd.DataFrame, granularity: str) -> pd.DataFrame:
    """é‡é‡‡æ ·æ•°æ®åˆ°ç›®æ ‡ç²’åº¦"""
    
    if granularity == "1min":
        return df  # åŸå§‹æ•°æ®å°±æ˜¯åˆ†é’Ÿçº§
    
    freq = GRANULARITY_TO_FREQ[granularity]
    logger.info(f"ğŸ“Š é‡é‡‡æ ·åˆ° {granularity} (freq={freq})...")
    
    # è®¡ç®—é¢„ä¼°ç»„æ•°
    if granularity == "1d":
        n_groups = int((df.index.max() - df.index.min()).days) + 1
    elif granularity == "4h":
        n_groups = int((df.index.max() - df.index.min()).total_seconds() / 14400) + 1
    elif granularity == "1h":
        n_groups = int((df.index.max() - df.index.min()).total_seconds() / 3600) + 1
    else:
        # åˆ†é’Ÿçº§
        minutes = int(granularity.replace("min", ""))
        n_groups = int((df.index.max() - df.index.min()).total_seconds() / (60 * minutes)) + 1
    
    # åˆ†ç»„é‡é‡‡æ ·
    iterator = df.groupby(pd.Grouper(freq=freq))
    if tqdm:
        iterator = tqdm(iterator, total=n_groups, desc=f"é‡é‡‡æ ·åˆ° {granularity}", unit="bar")
    
    records = []
    has_volume = "volume" in df.columns
    
    for ts, g in iterator:
        if g.empty:
            continue
        rec = {
            "timestamp": ts,
            "open": g["open"].iloc[0],
            "high": g["high"].max(),
            "low": g["low"].min(),
            "close": g["close"].iloc[-1]
        }
        if has_volume:
            rec["volume"] = g["volume"].sum()
        records.append(rec)
    
    if not records:
        logger.error("é‡é‡‡æ ·ç»“æœä¸ºç©º")
        return pd.DataFrame()
    
    df_resampled = pd.DataFrame(records).set_index("timestamp").sort_index()
    if not has_volume:
        df_resampled["volume"] = 0
    
    df_resampled = df_resampled.dropna()
    
    logger.info(f"âœ… é‡é‡‡æ ·å®Œæˆ: {len(df_resampled)} æ¡ {granularity} æ•°æ®")
    logger.info(f"   æ—¶é—´èŒƒå›´: {df_resampled.index.min()} ~ {df_resampled.index.max()}")
    
    return df_resampled


def precompute_all_granularities(cache_dir: Optional[Path] = None):
    """
    é¢„è®¡ç®—æ‰€æœ‰ç²’åº¦çš„ç¼“å­˜ï¼ˆä¸€æ¬¡æ€§å¤„ç†ï¼‰
    
    è¿™ä¼šä¸‹è½½åŸå§‹æ•°æ®å¹¶ç”Ÿæˆæ‰€æœ‰ç²’åº¦çš„ç¼“å­˜æ–‡ä»¶ï¼Œ
    ä¹‹ååŠ è½½ä»»ä½•ç²’åº¦éƒ½ä¼šéå¸¸å¿«ã€‚
    """
    if cache_dir is None:
        cache_dir = Path(__file__).parent.parent.parent / "data" / "hf_cache"
    
    logger.info("=" * 50)
    logger.info("é¢„è®¡ç®—æ‰€æœ‰æ—¶é—´ç²’åº¦ç¼“å­˜")
    logger.info("=" * 50)
    
    # å…ˆåŠ è½½åŸå§‹æ•°æ®
    raw_cache = cache_dir / "hf_btc_raw.parquet"
    df_raw = _load_raw_hf_data(raw_cache, force_reload=False)
    
    if df_raw.empty:
        logger.error("æ— æ³•åŠ è½½åŸå§‹æ•°æ®")
        return
    
    # ç”Ÿæˆå„ç²’åº¦ç¼“å­˜
    for granularity in VALID_GRANULARITIES:
        logger.info(f"\nå¤„ç† {granularity}...")
        cache_file = cache_dir / f"hf_btc_{granularity}.parquet"
        
        if cache_file.exists():
            logger.info(f"  å·²å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        df = _resample_data(df_raw, granularity)
        df.to_parquet(cache_file)
        logger.info(f"  âœ… å·²ä¿å­˜: {cache_file}")
    
    logger.info("\n" + "=" * 50)
    logger.info("âœ… æ‰€æœ‰ç²’åº¦ç¼“å­˜ç”Ÿæˆå®Œæˆ!")
    logger.info("=" * 50)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•åŠ è½½ä¸åŒç²’åº¦
    for g in ["15min", "1h", "4h"]:
        df = load_hf_btc_multi_granularity(granularity=g)
        if not df.empty:
            print(f"\n{g}: {len(df)} æ¡è®°å½•")
            print(f"  èŒƒå›´: {df.index.min()} ~ {df.index.max()}")
