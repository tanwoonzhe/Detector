"""
Binance å†å²æ•°æ®å½’æ¡£ä¸‹è½½å™¨
================================
ä» Binance Data Vision ä¸‹è½½å®Œæ•´å†å²Kçº¿æ•°æ®

æ•°æ®æº: https://data.binance.vision/
æ”¯æŒç²’åº¦: 1m, 3m, 5m, 15m, 30m, 1h, 2h, 4h, 6h, 8h, 12h, 1d

ç‰¹ç‚¹:
- å®Œæ•´å†å²æ•°æ® (2017å¹´è‡³ä»Š)
- å®˜æ–¹æ•°æ®æºï¼Œè´¨é‡é«˜
- æ”¯æŒæ‰¹é‡ä¸‹è½½å’Œæœ¬åœ°ç¼“å­˜
"""

import asyncio
import aiohttp
import logging
import zipfile
import io
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, List, Tuple
import pandas as pd
from tqdm import tqdm

logger = logging.getLogger(__name__)

# Binance Data Vision åŸºç¡€URL
BASE_URL = "https://data.binance.vision/data/spot"

# æ”¯æŒçš„æ—¶é—´ç²’åº¦
VALID_INTERVALS = ["1m", "3m", "5m", "15m", "30m", "1h", "2h", "4h", "6h", "8h", "12h", "1d"]


class BinanceHistoricalFetcher:
    """
    Binance å†å²æ•°æ®ä¸‹è½½å™¨
    
    ä» Binance Data Vision ä¸‹è½½æœˆåº¦/æ—¥åº¦å½’æ¡£Kçº¿æ•°æ®
    """
    
    def __init__(self, cache_dir: Optional[Path] = None):
        """
        åˆå§‹åŒ–
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º data/raw/binance_historical/
        """
        if cache_dir is None:
            cache_dir = Path(__file__).parent.parent.parent / "data" / "raw" / "binance_historical"
        
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._session: Optional[aiohttp.ClientSession] = None
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """è·å–HTTPä¼šè¯"""
        if self._session is None or self._session.closed:
            timeout = aiohttp.ClientTimeout(total=300)  # 5åˆ†é’Ÿè¶…æ—¶
            self._session = aiohttp.ClientSession(timeout=timeout)
        return self._session
    
    async def close(self):
        """å…³é—­ä¼šè¯"""
        if self._session and not self._session.closed:
            await self._session.close()
    
    def _get_monthly_url(self, symbol: str, interval: str, year: int, month: int) -> str:
        """ç”Ÿæˆæœˆåº¦æ•°æ®URL"""
        return f"{BASE_URL}/monthly/klines/{symbol}/{interval}/{symbol}-{interval}-{year}-{month:02d}.zip"
    
    def _get_daily_url(self, symbol: str, interval: str, date: datetime) -> str:
        """ç”Ÿæˆæ—¥åº¦æ•°æ®URL"""
        date_str = date.strftime("%Y-%m-%d")
        return f"{BASE_URL}/daily/klines/{symbol}/{interval}/{symbol}-{interval}-{date_str}.zip"
    
    async def _download_zip(self, url: str) -> Optional[bytes]:
        """ä¸‹è½½ZIPæ–‡ä»¶"""
        session = await self._get_session()
        
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    return await response.read()
                elif response.status == 404:
                    return None  # æ–‡ä»¶ä¸å­˜åœ¨
                else:
                    logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: HTTP {response.status}")
                    return None
        except Exception as e:
            logger.warning(f"ä¸‹è½½å¼‚å¸¸ {url}: {e}")
            return None
    
    def _parse_kline_csv(self, csv_content: str) -> pd.DataFrame:
        """è§£æKçº¿CSVæ•°æ®"""
        from io import StringIO
        
        # Binance Kçº¿åˆ—: Open time, Open, High, Low, Close, Volume, Close time, 
        # Quote volume, Number of trades, Taker buy base, Taker buy quote, Ignore
        columns = [
            "open_time", "open", "high", "low", "close", "volume",
            "close_time", "quote_volume", "trades", "taker_buy_base",
            "taker_buy_quote", "ignore"
        ]
        
        df = pd.read_csv(StringIO(csv_content), names=columns, header=None)
        
        # è½¬æ¢æ—¶é—´æˆ³
        df["timestamp"] = pd.to_datetime(df["open_time"], unit="ms")
        
        # è½¬æ¢æ•°å€¼ç±»å‹
        for col in ["open", "high", "low", "close", "volume"]:
            df[col] = pd.to_numeric(df[col], errors="coerce")
        
        # åªä¿ç•™éœ€è¦çš„åˆ—
        df = df[["timestamp", "open", "high", "low", "close", "volume"]].copy()
        df.set_index("timestamp", inplace=True)
        
        return df
    
    async def download_monthly_data(
        self,
        symbol: str = "BTCUSDT",
        interval: str = "1h",
        start_year: int = 2017,
        start_month: int = 8,
        end_year: Optional[int] = None,
        end_month: Optional[int] = None,
        show_progress: bool = True
    ) -> pd.DataFrame:
        """
        ä¸‹è½½æœˆåº¦Kçº¿æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹ (å¦‚ BTCUSDT)
            interval: æ—¶é—´ç²’åº¦ (1m, 5m, 15m, 30m, 1h, 4h, 1d)
            start_year: å¼€å§‹å¹´ä»½
            start_month: å¼€å§‹æœˆä»½
            end_year: ç»“æŸå¹´ä»½ (é»˜è®¤å½“å‰å¹´)
            end_month: ç»“æŸæœˆä»½ (é»˜è®¤å½“å‰æœˆ)
            show_progress: æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        if interval not in VALID_INTERVALS:
            raise ValueError(f"æ— æ•ˆçš„æ—¶é—´ç²’åº¦: {interval}ï¼Œæ”¯æŒ: {VALID_INTERVALS}")
        
        if end_year is None:
            end_year = datetime.now().year
        if end_month is None:
            end_month = datetime.now().month
        
        # ç”Ÿæˆæœˆä»½åˆ—è¡¨
        months: List[Tuple[int, int]] = []
        year, month = start_year, start_month
        while (year, month) <= (end_year, end_month):
            months.append((year, month))
            month += 1
            if month > 12:
                month = 1
                year += 1
        
        logger.info(f"ğŸ“¥ ä¸‹è½½ {symbol} {interval} æ•°æ®: {start_year}-{start_month:02d} ~ {end_year}-{end_month:02d}")
        logger.info(f"   å…± {len(months)} ä¸ªæœˆ")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_file = self.cache_dir / f"{symbol}_{interval}_monthly.parquet"
        
        all_dfs = []
        
        # ä¸‹è½½è¿›åº¦æ¡
        iterator = tqdm(months, desc=f"ä¸‹è½½ {symbol} {interval}") if show_progress else months
        
        for year, month in iterator:
            # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
            month_cache = self.cache_dir / f"{symbol}_{interval}_{year}-{month:02d}.csv"
            
            if month_cache.exists():
                df = pd.read_csv(month_cache, parse_dates=["timestamp"], index_col="timestamp")
                all_dfs.append(df)
                continue
            
            # ä¸‹è½½
            url = self._get_monthly_url(symbol, interval, year, month)
            zip_data = await self._download_zip(url)
            
            if zip_data is None:
                continue
            
            # è§£å‹å¹¶è§£æ
            try:
                with zipfile.ZipFile(io.BytesIO(zip_data)) as zf:
                    for name in zf.namelist():
                        if name.endswith(".csv"):
                            csv_content = zf.read(name).decode("utf-8")
                            df = self._parse_kline_csv(csv_content)
                            
                            # ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜
                            df.to_csv(month_cache)
                            all_dfs.append(df)
                            break
            except Exception as e:
                logger.warning(f"è§£æå¤±è´¥ {year}-{month:02d}: {e}")
        
        if not all_dfs:
            logger.warning("æ²¡æœ‰ä¸‹è½½åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        df_merged = pd.concat(all_dfs)
        df_merged.sort_index(inplace=True)
        df_merged = df_merged[~df_merged.index.duplicated(keep="first")]
        
        # ä¿å­˜åˆå¹¶åçš„parquet
        df_merged.to_parquet(cache_file)
        
        logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {len(df_merged)} æ¡è®°å½•")
        logger.info(f"   æ—¶é—´èŒƒå›´: {df_merged.index.min()} ~ {df_merged.index.max()}")
        logger.info(f"   ç¼“å­˜ä½ç½®: {cache_file}")
        
        return df_merged
    
    def load_cached_data(
        self,
        symbol: str = "BTCUSDT",
        interval: str = "1h"
    ) -> pd.DataFrame:
        """
        åŠ è½½æœ¬åœ°ç¼“å­˜æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹
            interval: æ—¶é—´ç²’åº¦
            
        Returns:
            DataFrame
        """
        cache_file = self.cache_dir / f"{symbol}_{interval}_monthly.parquet"
        
        if cache_file.exists():
            logger.info(f"ğŸ“ åŠ è½½ç¼“å­˜: {cache_file}")
            df = pd.read_parquet(cache_file)
            logger.info(f"   {len(df)} æ¡è®°å½•, {df.index.min()} ~ {df.index.max()}")
            return df
        
        # å°è¯•åˆå¹¶æœˆåº¦CSV
        csv_files = sorted(self.cache_dir.glob(f"{symbol}_{interval}_*.csv"))
        if csv_files:
            logger.info(f"ğŸ“ åˆå¹¶ {len(csv_files)} ä¸ªæœˆåº¦æ–‡ä»¶...")
            dfs = []
            for f in csv_files:
                df = pd.read_csv(f, parse_dates=["timestamp"], index_col="timestamp")
                dfs.append(df)
            
            if dfs:
                df_merged = pd.concat(dfs)
                df_merged.sort_index(inplace=True)
                df_merged = df_merged[~df_merged.index.duplicated(keep="first")]
                df_merged.to_parquet(cache_file)
                return df_merged
        
        logger.warning(f"æœªæ‰¾åˆ°ç¼“å­˜æ•°æ®: {symbol} {interval}")
        return pd.DataFrame()
    
    async def download_recent_daily(
        self,
        symbol: str = "BTCUSDT",
        interval: str = "1h",
        days: int = 30
    ) -> pd.DataFrame:
        """
        ä¸‹è½½æœ€è¿‘Nå¤©çš„æ—¥åº¦æ•°æ®ï¼ˆç”¨äºè¡¥å……æœ€æ–°æ•°æ®ï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹
            interval: æ—¶é—´ç²’åº¦
            days: å¤©æ•°
            
        Returns:
            DataFrame
        """
        logger.info(f"ğŸ“¥ ä¸‹è½½ {symbol} {interval} æœ€è¿‘ {days} å¤©æ•°æ®...")
        
        all_dfs = []
        end_date = datetime.now()
        
        for i in range(days):
            date = end_date - timedelta(days=i)
            url = self._get_daily_url(symbol, interval, date)
            zip_data = await self._download_zip(url)
            
            if zip_data is None:
                continue
            
            try:
                with zipfile.ZipFile(io.BytesIO(zip_data)) as zf:
                    for name in zf.namelist():
                        if name.endswith(".csv"):
                            csv_content = zf.read(name).decode("utf-8")
                            df = self._parse_kline_csv(csv_content)
                            all_dfs.append(df)
                            break
            except Exception as e:
                logger.warning(f"è§£æå¤±è´¥ {date.date()}: {e}")
        
        if not all_dfs:
            return pd.DataFrame()
        
        df_merged = pd.concat(all_dfs)
        df_merged.sort_index(inplace=True)
        df_merged = df_merged[~df_merged.index.duplicated(keep="first")]
        
        logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {len(df_merged)} æ¡è®°å½•")
        
        return df_merged


async def download_btc_historical(
    interval: str = "1h",
    start_year: int = 2017,
    show_progress: bool = True
) -> pd.DataFrame:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸‹è½½BTCå†å²æ•°æ®
    
    Args:
        interval: æ—¶é—´ç²’åº¦ (1m, 5m, 15m, 30m, 1h, 4h, 1d)
        start_year: å¼€å§‹å¹´ä»½
        show_progress: æ˜¾ç¤ºè¿›åº¦æ¡
        
    Returns:
        DataFrame with OHLCV data
    """
    fetcher = BinanceHistoricalFetcher()
    
    try:
        df = await fetcher.download_monthly_data(
            symbol="BTCUSDT",
            interval=interval,
            start_year=start_year,
            start_month=8 if start_year == 2017 else 1,
            show_progress=show_progress
        )
        return df
    finally:
        await fetcher.close()


def load_btc_historical(interval: str = "1h") -> pd.DataFrame:
    """
    ä¾¿æ·å‡½æ•°ï¼šåŠ è½½æœ¬åœ°ç¼“å­˜çš„BTCå†å²æ•°æ®
    
    Args:
        interval: æ—¶é—´ç²’åº¦
        
    Returns:
        DataFrame
    """
    fetcher = BinanceHistoricalFetcher()
    return fetcher.load_cached_data(symbol="BTCUSDT", interval=interval)
