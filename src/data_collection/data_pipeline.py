"""
å¤šæ•°æ®æºåˆå¹¶ç®¡é“
================================
ç»Ÿä¸€è·å–ã€åˆå¹¶ã€å¯¹é½æ¥è‡ªå¤šä¸ªæ•°æ®æºçš„æ•°æ®

æ”¯æŒçš„æ•°æ®æº:
- CoinGecko: åŠ å¯†è´§å¸ä»·æ ¼/å¸‚åœºæ•°æ®
- FMP: å®è§‚ç»æµ/è‚¡ç¥¨æŒ‡æ•°/å•†å“/å¤–æ±‡/æ–°é—»
- CoinMetrics: é“¾ä¸Šæ•°æ®/ç½‘ç»œæŒ‡æ ‡
- HuggingFace: å†å²æ•°æ®é›†

ä½¿ç”¨æ–¹æ³•:
    pipeline = DataPipeline(fmp_api_key="your_key")
    df = await pipeline.fetch_all(days=90)
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any
import pandas as pd
import numpy as np

from .coingecko_fetcher import CoinGeckoFetcher
from .fmp_fetcher import FMPFetcher
from .coinmetrics_fetcher import CoinMetricsFetcher

logger = logging.getLogger(__name__)


class DataPipeline:
    """
    å¤šæ•°æ®æºåˆå¹¶ç®¡é“
    
    ç»Ÿä¸€ç®¡ç†å¤šä¸ªæ•°æ®æºçš„è·å–å’Œåˆå¹¶ï¼Œç”Ÿæˆç”¨äºè®­ç»ƒçš„ç‰¹å¾çŸ©é˜µ
    """
    
    def __init__(
        self,
        fmp_api_key: str = "",
        coinmetrics_api_key: str = "",
        use_cache: bool = True
    ):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡é“
        
        Args:
            fmp_api_key: FMP APIå¯†é’¥
            coinmetrics_api_key: CoinMetrics APIå¯†é’¥ï¼ˆç¤¾åŒºç‰ˆå¯ä¸ºç©ºï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        """
        self.fmp_api_key = fmp_api_key
        self.coinmetrics_api_key = coinmetrics_api_key
        self.use_cache = use_cache
        
        # åˆå§‹åŒ–å„æ•°æ®æº
        self._coingecko: Optional[CoinGeckoFetcher] = None
        self._fmp: Optional[FMPFetcher] = None
        self._coinmetrics: Optional[CoinMetricsFetcher] = None
    
    @property
    def coingecko(self) -> CoinGeckoFetcher:
        if self._coingecko is None:
            self._coingecko = CoinGeckoFetcher()
        return self._coingecko
    
    @property
    def fmp(self) -> FMPFetcher:
        if self._fmp is None:
            self._fmp = FMPFetcher(api_key=self.fmp_api_key)
        return self._fmp
    
    @property
    def coinmetrics(self) -> CoinMetricsFetcher:
        if self._coinmetrics is None:
            self._coinmetrics = CoinMetricsFetcher(api_key=self.coinmetrics_api_key)
        return self._coinmetrics
    
    async def close(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        if self._coingecko:
            await self._coingecko.close()
        if self._fmp:
            await self._fmp.close()
        if self._coinmetrics:
            await self._coinmetrics.close()
    
    # ==================== æ•°æ®è·å–æ–¹æ³• ====================
    
    async def fetch_btc_price(self, days: int = 90) -> pd.DataFrame:
        """
        è·å–BTCä»·æ ¼æ•°æ® (OHLCV)
        
        ä¼˜å…ˆä½¿ç”¨CoinGeckoï¼Œå¤±è´¥åˆ™å°è¯•FMP
        """
        logger.info("ğŸ“Š è·å–BTCä»·æ ¼æ•°æ®...")
        
        try:
            market_data = await self.coingecko.get_hourly_ohlcv(
                symbol="bitcoin",
                days=days
            )
            df = market_data.to_dataframe()
            if not df.empty:
                logger.info(f"âœ… CoinGeckoä»·æ ¼æ•°æ®: {len(df)} æ¡")
                return df
        except Exception as e:
            logger.warning(f"CoinGeckoè·å–å¤±è´¥: {e}")
        
        # å›é€€åˆ°FMP
        if self.fmp_api_key:
            try:
                market_data = await self.fmp.get_hourly_ohlcv(
                    symbol="BTCUSD",
                    days=days
                )
                df = market_data.to_dataframe()
                if not df.empty:
                    logger.info(f"âœ… FMPä»·æ ¼æ•°æ®: {len(df)} æ¡")
                    return df
            except Exception as e:
                logger.warning(f"FMPè·å–å¤±è´¥: {e}")
        
        return pd.DataFrame()
    
    async def fetch_macro_data(self, days: int = 365) -> pd.DataFrame:
        """
        è·å–å®è§‚ç»æµæ•°æ®
        
        åŒ…å«: å›½å€ºæ”¶ç›Šç‡ã€è‚¡ç¥¨æŒ‡æ•°ã€VIXã€é»„é‡‘ã€ç¾å…ƒæŒ‡æ•°ç­‰
        """
        if not self.fmp_api_key:
            logger.warning("âš ï¸ FMP APIå¯†é’¥æœªè®¾ç½®ï¼Œè·³è¿‡å®è§‚æ•°æ®")
            return pd.DataFrame()
        
        logger.info("ğŸ“Š è·å–å®è§‚ç»æµæ•°æ®...")
        
        dfs = []
        
        # 1. å›½å€ºæ”¶ç›Šç‡
        try:
            df_treasury = await self.fmp.get_treasury_rates()
            if not df_treasury.empty:
                dfs.append(df_treasury)
                logger.info(f"  âœ“ å›½å€ºæ”¶ç›Šç‡: {len(df_treasury)} æ¡")
        except Exception as e:
            logger.warning(f"  âœ— å›½å€ºæ”¶ç›Šç‡å¤±è´¥: {e}")
        
        # 2. è‚¡ç¥¨æŒ‡æ•°
        indices = [
            ("^GSPC", "sp500"),    # S&P 500
            ("^VIX", "vix"),       # VIXææ…ŒæŒ‡æ•°
        ]
        
        for symbol, name in indices:
            try:
                df_idx = await self.fmp.get_index_data(symbol=symbol, days=days)
                if not df_idx.empty:
                    dfs.append(df_idx)
                    logger.info(f"  âœ“ {name}: {len(df_idx)} æ¡")
            except Exception as e:
                logger.warning(f"  âœ— {name}å¤±è´¥: {e}")
        
        # 3. é»„é‡‘
        try:
            df_gold = await self.fmp.get_commodity_data(symbol="GCUSD", days=days)
            if not df_gold.empty:
                dfs.append(df_gold)
                logger.info(f"  âœ“ é»„é‡‘: {len(df_gold)} æ¡")
        except Exception as e:
            logger.warning(f"  âœ— é»„é‡‘å¤±è´¥: {e}")
        
        # 4. ç¾å…ƒæŒ‡æ•°
        try:
            df_dxy = await self.fmp.get_forex_data(symbol="DXY", days=days)
            if not df_dxy.empty:
                dfs.append(df_dxy)
                logger.info(f"  âœ“ ç¾å…ƒæŒ‡æ•°: {len(df_dxy)} æ¡")
        except Exception as e:
            logger.warning(f"  âœ— ç¾å…ƒæŒ‡æ•°å¤±è´¥: {e}")
        
        if not dfs:
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰å®è§‚æ•°æ®
        df_macro = self._merge_dataframes(dfs)
        logger.info(f"âœ… å®è§‚æ•°æ®åˆå¹¶: {len(df_macro)} æ¡, {len(df_macro.columns)} åˆ—")
        
        return df_macro
    
    async def fetch_onchain_data(self, days: int = 90) -> pd.DataFrame:
        """
        è·å–é“¾ä¸Šæ•°æ®
        
        åŒ…å«: æ´»è·ƒåœ°å€ã€äº¤æ˜“æ•°ã€å“ˆå¸Œç‡ã€NVTç­‰
        """
        logger.info("ğŸ“Š è·å–é“¾ä¸Šæ•°æ®...")
        
        try:
            df_onchain = await self.coinmetrics.get_network_metrics(
                asset="btc",
                start_time=datetime.utcnow() - timedelta(days=days),
                end_time=datetime.utcnow()
            )
            
            if not df_onchain.empty:
                logger.info(f"âœ… é“¾ä¸Šæ•°æ®: {len(df_onchain)} æ¡, {len(df_onchain.columns)} åˆ—")
                return df_onchain
                
        except Exception as e:
            logger.warning(f"é“¾ä¸Šæ•°æ®è·å–å¤±è´¥: {e}")
        
        return pd.DataFrame()
    
    async def fetch_cross_asset(self, days: int = 90) -> pd.DataFrame:
        """
        è·å–è·¨å¸‚åœºèµ„äº§æ•°æ®
        
        åŒ…å«: ETHã€ä¸»è¦altcoinsç­‰
        """
        logger.info("ğŸ“Š è·å–è·¨å¸‚åœºèµ„äº§æ•°æ®...")
        
        dfs = []
        
        # è·å–ETHä»·æ ¼
        try:
            market_data = await self.coingecko.get_hourly_ohlcv(
                symbol="ethereum",
                days=days
            )
            df_eth = market_data.to_dataframe()
            if not df_eth.empty:
                df_eth.columns = [f"eth_{col}" for col in df_eth.columns]
                dfs.append(df_eth)
                logger.info(f"  âœ“ ETH: {len(df_eth)} æ¡")
        except Exception as e:
            logger.warning(f"  âœ— ETHå¤±è´¥: {e}")
        
        if not dfs:
            return pd.DataFrame()
        
        df_cross = self._merge_dataframes(dfs)
        logger.info(f"âœ… è·¨å¸‚åœºæ•°æ®: {len(df_cross)} æ¡, {len(df_cross.columns)} åˆ—")
        
        return df_cross
    
    # ==================== ä¸»è¦æ¥å£ ====================
    
    async def fetch_all(
        self,
        days: int = 90,
        include_macro: bool = True,
        include_onchain: bool = True,
        include_cross_asset: bool = True,
        resample_to_hourly: bool = True
    ) -> pd.DataFrame:
        """
        è·å–æ‰€æœ‰æ•°æ®å¹¶åˆå¹¶
        
        Args:
            days: å†å²å¤©æ•°
            include_macro: æ˜¯å¦åŒ…å«å®è§‚æ•°æ®
            include_onchain: æ˜¯å¦åŒ…å«é“¾ä¸Šæ•°æ®
            include_cross_asset: æ˜¯å¦åŒ…å«è·¨å¸‚åœºæ•°æ®
            resample_to_hourly: æ˜¯å¦é‡é‡‡æ ·åˆ°å°æ—¶çº§åˆ«
            
        Returns:
            åˆå¹¶åçš„DataFrameï¼Œä»¥timestampä¸ºç´¢å¼•
        """
        logger.info("=" * 50)
        logger.info("å¼€å§‹è·å–å¤šæºæ•°æ®...")
        logger.info("=" * 50)
        
        # 1. è·å–BTCä»·æ ¼æ•°æ®ï¼ˆæ ¸å¿ƒï¼‰
        df_btc = await self.fetch_btc_price(days=days)
        
        if df_btc.empty:
            logger.error("âŒ BTCä»·æ ¼æ•°æ®è·å–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return pd.DataFrame()
        
        dfs_to_merge = [df_btc]
        
        # 2. å¹¶è¡Œè·å–å…¶ä»–æ•°æ®
        tasks = []
        
        if include_macro and self.fmp_api_key:
            tasks.append(("macro", self.fetch_macro_data(days=days)))
        
        if include_onchain:
            tasks.append(("onchain", self.fetch_onchain_data(days=days)))
        
        if include_cross_asset:
            tasks.append(("cross_asset", self.fetch_cross_asset(days=days)))
        
        if tasks:
            # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            results = await asyncio.gather(
                *[task[1] for task in tasks],
                return_exceptions=True
            )
            
            for (name, _), result in zip(tasks, results):
                if isinstance(result, Exception):
                    logger.warning(f"âš ï¸ {name} è·å–å¤±è´¥: {result}")
                elif isinstance(result, pd.DataFrame) and not result.empty:
                    dfs_to_merge.append(result)
        
        # 3. åˆå¹¶æ‰€æœ‰æ•°æ®
        logger.info("\nğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®æº...")
        df_merged = self._merge_dataframes(dfs_to_merge, resample_to_hourly=resample_to_hourly)
        
        # 4. æ¸…ç†æ•°æ®
        df_merged = self._clean_data(df_merged)
        
        logger.info("=" * 50)
        logger.info(f"âœ… æ•°æ®è·å–å®Œæˆ!")
        logger.info(f"   æ€»è¡Œæ•°: {len(df_merged)}")
        logger.info(f"   æ€»åˆ—æ•°: {len(df_merged.columns)}")
        logger.info(f"   æ—¶é—´èŒƒå›´: {df_merged.index.min()} ~ {df_merged.index.max()}")
        logger.info("=" * 50)
        
        return df_merged
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _merge_dataframes(
        self,
        dfs: List[pd.DataFrame],
        resample_to_hourly: bool = False
    ) -> pd.DataFrame:
        """
        åˆå¹¶å¤šä¸ªDataFrame
        
        Args:
            dfs: DataFrameåˆ—è¡¨
            resample_to_hourly: æ˜¯å¦é‡é‡‡æ ·åˆ°å°æ—¶çº§åˆ«
            
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        if not dfs:
            return pd.DataFrame()
        
        if len(dfs) == 1:
            return dfs[0]
        
        # ç¡®ä¿æ‰€æœ‰ç´¢å¼•æ˜¯datetimeç±»å‹ä¸”æ— æ—¶åŒº
        processed_dfs = []
        for df in dfs:
            if df.empty:
                continue
            
            df = df.copy()
            
            # ç¡®ä¿ç´¢å¼•æ˜¯datetime
            if not isinstance(df.index, pd.DatetimeIndex):
                if "timestamp" in df.columns:
                    df.set_index("timestamp", inplace=True)
                else:
                    continue
            
            # ç§»é™¤æ—¶åŒº
            if isinstance(df.index, pd.DatetimeIndex) and df.index.tz is not None:
                df.index = df.index.tz_localize(None)  # type: ignore
            
            # å¦‚æœéœ€è¦é‡é‡‡æ ·æ—¥çº§æ•°æ®åˆ°å°æ—¶çº§ï¼ˆå‰å‘å¡«å……ï¼‰
            if resample_to_hourly:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¥çº§æ•°æ®
                if len(df) > 1:
                    time_diff = (df.index[1] - df.index[0]).total_seconds()
                    if time_diff >= 86400:  # æ—¥çº§æˆ–æ›´é•¿
                        df = df.resample('h').ffill()
            
            processed_dfs.append(df)
        
        if not processed_dfs:
            return pd.DataFrame()
        
        # ä½¿ç”¨outer joinåˆå¹¶
        df_merged = processed_dfs[0]
        for df in processed_dfs[1:]:
            df_merged = df_merged.join(df, how='outer')
        
        df_merged.sort_index(inplace=True)
        
        return df_merged
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…ç†æ•°æ®
        
        - åˆ é™¤å…¨ä¸ºNaNçš„è¡Œ
        - å‰å‘å¡«å……ç¼ºå¤±å€¼
        - åˆ é™¤ä»æœ‰NaNçš„è¡Œ
        """
        if df.empty:
            return df
        
        df = df.copy()
        
        # åˆ é™¤å…¨ä¸ºNaNçš„è¡Œ
        df.dropna(how='all', inplace=True)
        
        # å‰å‘å¡«å……
        df.ffill(inplace=True)
        
        # åå‘å¡«å……å‰©ä½™çš„NaNï¼ˆå¼€å¤´éƒ¨åˆ†ï¼‰
        df.bfill(inplace=True)
        
        # åˆ é™¤ä»æœ‰NaNçš„è¡Œ
        df.dropna(inplace=True)
        
        return df
    
    def get_feature_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        è·å–ç‰¹å¾æ‘˜è¦
        
        Args:
            df: æ•°æ®DataFrame
            
        Returns:
            ç‰¹å¾æ‘˜è¦å­—å…¸
        """
        if df.empty:
            return {}
        
        # æŒ‰ç±»åˆ«åˆ†ç»„åˆ—
        categories = {
            "price": [c for c in df.columns if c in ["open", "high", "low", "close", "volume"]],
            "macro": [c for c in df.columns if any(x in c for x in ["treasury", "sp500", "vix", "dxy", "gc_"])],
            "onchain": [c for c in df.columns if c.startswith("cm_")],
            "cross_asset": [c for c in df.columns if c.startswith("eth_")],
        }
        
        summary = {
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "time_range": {
                "start": str(df.index.min()),
                "end": str(df.index.max())
            },
            "categories": {
                name: {
                    "count": len(cols),
                    "columns": cols
                }
                for name, cols in categories.items()
            },
            "missing_values": df.isna().sum().to_dict()
        }
        
        return summary


# ==================== ä¾¿æ·å‡½æ•° ====================

async def fetch_training_data(
    fmp_api_key: str = "",
    days: int = 90,
    include_macro: bool = True,
    include_onchain: bool = True
) -> pd.DataFrame:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–è®­ç»ƒæ•°æ®
    
    Args:
        fmp_api_key: FMP APIå¯†é’¥
        days: å†å²å¤©æ•°
        include_macro: æ˜¯å¦åŒ…å«å®è§‚æ•°æ®
        include_onchain: æ˜¯å¦åŒ…å«é“¾ä¸Šæ•°æ®
        
    Returns:
        åˆå¹¶åçš„è®­ç»ƒæ•°æ®DataFrame
    """
    pipeline = DataPipeline(fmp_api_key=fmp_api_key)
    
    try:
        df = await pipeline.fetch_all(
            days=days,
            include_macro=include_macro,
            include_onchain=include_onchain
        )
        return df
    finally:
        await pipeline.close()
